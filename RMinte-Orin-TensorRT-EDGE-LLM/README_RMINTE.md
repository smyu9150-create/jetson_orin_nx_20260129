# RMinte-Orin-TensorRT-EDGE-LLM

åŸºäº [NVIDIA TensorRT-Edge-LLM](https://github.com/NVIDIA/TensorRT-Edge-LLM) çš„ **Jetson AGX Orin** é«˜æ€§èƒ½ VLM æ¨ç†æ–¹æ¡ˆï¼Œä¸“ä¸ºè¾¹ç¼˜è®¾å¤‡ä¸Šçš„è§†è§‰è¯­è¨€æ¨¡å‹éƒ¨ç½²è€Œä¼˜åŒ–ã€‚

## ğŸš€ ä¸»è¦ç‰¹æ€§

- **OpenAI å…¼å®¹ API æœåŠ¡å™¨** - å®Œæ•´æ”¯æŒ `/v1/chat/completions` ç«¯ç‚¹
- **çœŸæ­£çš„æµå¼è¾“å‡º** - Token çº§åˆ«çš„ SSE æµå¼å“åº”ï¼Œæ”¯æŒ UTF-8 å¤šå­—èŠ‚å­—ç¬¦
- **Base64 å›¾ç‰‡æ”¯æŒ** - ç›´æ¥å¤„ç†å‰ç«¯ä¼ æ¥çš„ Base64 ç¼–ç å›¾ç‰‡
- **è‡ªåŠ¨å›¾ç‰‡ç¼©æ”¾** - è‡ªåŠ¨è°ƒæ•´å¤§å›¾ä»¥é€‚åº” token é™åˆ¶
- **æŒä¹…åŒ–å¼•æ“** - å¼•æ“ä¸€æ¬¡åŠ è½½ï¼Œå¸¸é©»å†…å­˜ï¼Œé¿å…é‡å¤åŠ è½½
- **INT4-AWQ é‡åŒ–æ”¯æŒ** - æ”¯æŒé‡åŒ–æ¨¡å‹ä»¥å‡å°‘æ˜¾å­˜å ç”¨

## ğŸ“¦ æ–°å¢ç»„ä»¶

### 1. OpenAI å…¼å®¹ HTTP æœåŠ¡å™¨

**æ–‡ä»¶ä½ç½®**: `examples/server/llm_server.cpp`

åŠŸèƒ½ç‰¹æ€§:
- å®Œæ•´çš„ OpenAI Chat Completions API å…¼å®¹
- æ”¯æŒæµå¼ (SSE) å’Œéæµå¼å“åº”
- æ”¯æŒå¤šæ¨¡æ€è¾“å…¥ (æ–‡æœ¬ + å›¾ç‰‡)
- Base64 å›¾ç‰‡æ•°æ® URL è‡ªåŠ¨è§£ç 
- å¤§å›¾è‡ªåŠ¨ç¼©æ”¾ (é»˜è®¤æœ€å¤§ 896px)
- æŒä¹…åŒ–å¼•æ“ï¼Œå¯åŠ¨åå¸¸é©»å†…å­˜

### 2. æµå¼æ¨ç† API

**æ–‡ä»¶ä½ç½®**: `cpp/runtime/llmInferenceRuntime.cpp`

æ–°å¢ `handleRequestStreaming()` æ–¹æ³•:
- Token çº§åˆ«çš„å›è°ƒæœºåˆ¶
- UTF-8 å­—ç¬¦è¾¹ç•Œæ­£ç¡®å¤„ç†
- æ”¯æŒåœæ­¢è¯æ£€æµ‹

## ğŸ› ï¸ æ„å»ºæŒ‡å—

### ç¯å¢ƒè¦æ±‚

- **ç¡¬ä»¶**: Jetson AGX Orin (64GB æ¨è)
- **ç³»ç»Ÿ**: JetPack 6.2+ (L4T R36.4.x)
- **TensorRT**: 10.7+
- **CUDA**: 12.6

### ç¼–è¯‘æ­¥éª¤

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/thomas-hiddenpeak/RMinte-Orin-TensorRT-EDGE-LLM.git
cd RMinte-Orin-TensorRT-EDGE-LLM

# åˆ›å»ºæ„å»ºç›®å½•
mkdir -p build && cd build

# é…ç½® CMake (Jetson AGX Orin)
cmake .. -DTRT_PACKAGE_DIR=/usr -DCUDA_VERSION=12.6 -DCMAKE_CUDA_ARCHITECTURES=87

# ç¼–è¯‘
make -j$(nproc)
```

### ç¼–è¯‘äº§ç‰©

- `build/examples/llm/llm_build` - LLM å¼•æ“æ„å»ºå·¥å…·
- `build/examples/multimodal/visual_build` - è§†è§‰ç¼–ç å™¨å¼•æ“æ„å»ºå·¥å…·
- `build/examples/server/llm_server` - OpenAI å…¼å®¹ API æœåŠ¡å™¨

## ğŸ“– ä½¿ç”¨æŒ‡å—

### 1. å¯¼å‡º ONNX æ¨¡å‹

```bash
# å®‰è£… Python åŒ…
pip install -e .

# å¯¼å‡º LLM
tensorrt-edgellm-export-llm \
  --model=/path/to/Qwen3-VL-8B-Instruct \
  --output=/path/to/onnx/llm/Qwen3-VL-8B-Instruct

# å¯¼å‡ºè§†è§‰ç¼–ç å™¨
tensorrt-edgellm-export-visual \
  --model=/path/to/Qwen3-VL-8B-Instruct \
  --output=/path/to/onnx/visual/Qwen3-VL-8B-Instruct
```

### 2. æ„å»º TensorRT å¼•æ“

```bash
cd build
export EDGELLM_PLUGIN_PATH=$PWD/libNvInfer_edgellm_plugin.so

# æ„å»º LLM å¼•æ“
./examples/llm/llm_build \
  --onnxDir=/path/to/onnx/llm/Qwen3-VL-8B-Instruct \
  --engineDir=/path/to/engine/llm/Qwen3-VL-8B-Instruct \
  --maxInputLen=16384 \
  --maxKVCacheCapacity=32768 \
  --maxBatchSize=4 \
  --vlm \
  --minImageTokens=256 \
  --maxImageTokens=4096

# æ„å»ºè§†è§‰ç¼–ç å™¨å¼•æ“
./examples/multimodal/visual_build \
  --onnxDir=/path/to/onnx/visual/Qwen3-VL-8B-Instruct \
  --engineDir=/path/to/engine/visual/Qwen3-VL-8B-Instruct
```

### 3. å¯åŠ¨ API æœåŠ¡å™¨

```bash
cd build
export EDGELLM_PLUGIN_PATH=$PWD/libNvInfer_edgellm_plugin.so

./examples/server/llm_server \
  --engineDir=/path/to/engine/llm/Qwen3-VL-8B-Instruct \
  --multimodalEngineDir=/path/to/engine/visual/Qwen3-VL-8B-Instruct \
  --port=58010 \
  --debug
```

### 4. API ä½¿ç”¨ç¤ºä¾‹

#### æ–‡æœ¬å¯¹è¯

```bash
curl -X POST http://localhost:58010/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-vl",
    "messages": [
      {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}
    ],
    "stream": true
  }'
```

#### å›¾ç‰‡ç†è§£ (URL)

```bash
curl -X POST http://localhost:58010/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-vl",
    "messages": [
      {
        "role": "user",
        "content": [
          {"type": "image_url", "image_url": {"url": "https://example.com/image.jpg"}},
          {"type": "text", "text": "æè¿°è¿™å¼ å›¾ç‰‡"}
        ]
      }
    ],
    "stream": true
  }'
```

#### å›¾ç‰‡ç†è§£ (Base64)

```bash
curl -X POST http://localhost:58010/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-vl",
    "messages": [
      {
        "role": "user",
        "content": [
          {"type": "image_url", "image_url": {"url": "data:image/jpeg;base64,/9j/4AAQ..."}},
          {"type": "text", "text": "è¿™æ˜¯ä»€ä¹ˆ?"}
        ]
      }
    ],
    "stream": true
  }'
```

## âš™ï¸ é…ç½®å‚æ•°è¯´æ˜

### å¼•æ“æ„å»ºå‚æ•°

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ (64GB Orin) |
|------|------|-------------------|
| `--maxInputLen` | æœ€å¤§è¾“å…¥é•¿åº¦ (tokens) | 16384 |
| `--maxKVCacheCapacity` | KV ç¼“å­˜å®¹é‡ (tokens) | 32768 |
| `--maxBatchSize` | æœ€å¤§æ‰¹å¤„ç†å¤§å° | 4 |
| `--maxImageTokens` | å•å›¾æœ€å¤§ tokens | 4096 |
| `--minImageTokens` | å•å›¾æœ€å° tokens | 256 |

### æœåŠ¡å™¨å‚æ•°

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `--engineDir` | LLM å¼•æ“ç›®å½• | (å¿…éœ€) |
| `--multimodalEngineDir` | è§†è§‰å¼•æ“ç›®å½• | (å¯é€‰) |
| `--port` | æœåŠ¡ç«¯å£ | 58010 |
| `--debug` | è°ƒè¯•æ¨¡å¼ | false |

## ğŸ“Š æ€§èƒ½å‚è€ƒ

åœ¨ Jetson AGX Orin 64GB ä¸Šæµ‹è¯• Qwen3-VL-8B-Instruct:

| é…ç½® | å¼•æ“å¤§å° | åŠ è½½æ—¶é—´ | Token é€Ÿåº¦ |
|------|---------|---------|-----------|
| FP16 | ~16 GB | ~28s | ~30 tok/s |
| INT4-AWQ | ~6 GB | ~14s | ~45 tok/s |

## ğŸ”§ ä¿®æ”¹çš„æ–‡ä»¶

ç›¸æ¯”åŸå§‹ TensorRT-Edge-LLMï¼Œæœ¬é¡¹ç›®ä¿®æ”¹/æ–°å¢äº†ä»¥ä¸‹æ–‡ä»¶:

### æ–°å¢æ–‡ä»¶
- `examples/server/llm_server.cpp` - OpenAI å…¼å®¹ API æœåŠ¡å™¨
- `examples/server/CMakeLists.txt` - æœåŠ¡å™¨æ„å»ºé…ç½®
- `.github/copilot-instructions.md` - é¡¹ç›®è¯´æ˜

### ä¿®æ”¹æ–‡ä»¶
- `cpp/runtime/llmInferenceRuntime.cpp` - æ·»åŠ  `handleRequestStreaming()` æµå¼æ¨ç†
- `cpp/runtime/llmInferenceRuntime.h` - æ·»åŠ æµå¼ API å£°æ˜
- `cpp/common/tensor.cpp` - Tensor å·¥å…·ä¿®å¤
- `examples/CMakeLists.txt` - æ·»åŠ  server å­ç›®å½•

## ğŸ¤ ä¸å‰ç«¯é›†æˆ

æœ¬æœåŠ¡å™¨å…¼å®¹ä»¥ä¸‹å®¢æˆ·ç«¯:

- **ChatBox** - æ¨èï¼Œå®Œç¾æ”¯æŒ
- **OpenWebUI** - æ”¯æŒ
- **å…¶ä»– OpenAI å…¼å®¹å®¢æˆ·ç«¯**

é…ç½®ç¤ºä¾‹ (ChatBox):
- API åœ°å€: `http://<orin-ip>:58010`
- API è·¯å¾„: `/v1/chat/completions`
- æ¨¡å‹åç§°: `qwen3-vl`

## ğŸ“ è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäº Apache-2.0 è®¸å¯è¯ï¼Œç»§æ‰¿è‡ª NVIDIA TensorRT-Edge-LLMã€‚

## ğŸ™ è‡´è°¢

- [NVIDIA TensorRT-Edge-LLM](https://github.com/NVIDIA/TensorRT-Edge-LLM) - åŸå§‹é¡¹ç›®
- [Qwen3-VL](https://github.com/QwenLM/Qwen2.5-VL) - è§†è§‰è¯­è¨€æ¨¡å‹
